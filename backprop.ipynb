{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Goal:</b> Train MNIST model by gradient-backpropagation without deep learning framework\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Notice:</b> Only uses numpy for array calculation. Tensorflow is just for loading mnist data</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from tensorflow import keras\n",
    "# without ONE-HOT encoding or normalization\n",
    "(x_train, y_train), (x_test,y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Global Hyperparameters:</b> Some variables that will remain constant throughout the code</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.05\n",
    "LEARNING_RATE_DECAY=0.5\n",
    "NUMBER_OF_EPOACHES = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Layer Classes:</b> Define classes for each layers, so we can edit model freely</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Input_layer:\n",
    "    def __init__(self, train_x):\n",
    "\n",
    "        if len(train_x.shape) == 3:\n",
    "            train_x = train_x.reshape(len(train_x), -1)\n",
    "\n",
    "        self.VALUE = train_x\n",
    "        self.SHAPE = train_x.shape\n",
    "        self.TYPE = 'INPUT'\n",
    "\n",
    "class Dense_layer:\n",
    "    def __init__(self, node_num):\n",
    "        self.NODE_SIZE = node_num\n",
    "\n",
    "        self.WEIGHT = None\n",
    "        self.BIAS = None\n",
    "\n",
    "        self.SHAPE = ((node_num, None), (node_num))\n",
    "        self.TYPE = 'DENSE'\n",
    "\n",
    "    def __call__(self, input):\n",
    "        self.WEIGHT = 0.1 * numpy.random.randn(input.shape[1], self.NODE_SIZE)\n",
    "        self.BIAS = 0.1 * numpy.random.randn(1,self.NODE_SIZE)\n",
    "\n",
    "class Activation_layer:\n",
    "    def __init__(self, function):\n",
    "        self.function = function\n",
    "        self.TYPE = 'ACTIVATION'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Model:</b> The forepropagation and backpropagation signal values for the layers are saved as dict items under corresponding key(layer name)</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Careful:</b> to keep it simple, there is one important assumption: <b>ONLY ONE SOFTMAX LAYER EXISTS IN ENTIRE LAYER AND IT ALWAYS COMES LAST.</b> The reason is because softmax combined with cross-entropy error results very simple and intuitive backpropagation result.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentaial_model:\n",
    "    def __init__(self, *list_of_layers):\n",
    "\n",
    "        self.model_layer = {} # layer weights\n",
    "        self.signal_forward = {} # forepropagation\n",
    "        self.signal_backward = {} # backpropagation\n",
    "\n",
    "        self.dense_layer_cnt = 0\n",
    "        self.activation_cnt = 0\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.input_layer = None # just a imaginary\n",
    "        \n",
    "        for layer in list_of_layers:\n",
    "            if layer.TYPE == 'INPUT':\n",
    "                self.model_layer['input_layer'] = layer.VALUE\n",
    "                self.input_layer = numpy.zeros((self.batch_size, layer.SHAPE[1]))\n",
    "\n",
    "            elif layer.TYPE == 'DENSE':\n",
    "                self.dense_layer_cnt += 1\n",
    "                layer(self.input_layer)\n",
    "                self.model_layer[f'dense_layer_{self.dense_layer_cnt}'] = [layer.WEIGHT, layer.BIAS]\n",
    "                self.input_layer = numpy.zeros((self.batch_size, layer.NODE_SIZE))\n",
    "\n",
    "            elif layer.TYPE == 'ACTIVATION':\n",
    "                self.activation_cnt += 1\n",
    "                self.model_layer[f'activation_layer_{layer.function}'] = layer.function\n",
    "\n",
    "    def predict(self, input):\n",
    "\n",
    "        self.signal_in = input\n",
    "        self.signal_depth = 0\n",
    "        for key in self.model_layer.keys():\n",
    "            if 'input' in key:\n",
    "                self.signal_forward[f'{key}'] = input\n",
    "                self.signal_in = self.signal_forward[f'{key}']\n",
    "            elif 'dense' in key:\n",
    "                self.signal_depth += 1\n",
    "                self.signal_forward[f'{key}'] = numpy.matmul(self.signal_in, self.model_layer[key][0]) + self.model_layer[key][1]\n",
    "                self.signal_in = self.signal_forward[f'{key}']\n",
    "\n",
    "            elif 'activation' in key:\n",
    "                self.signal_depth += 1\n",
    "                if 'Relu' in key:\n",
    "                    self.signal_forward[f'{key}'] = numpy.maximum(0, self.signal_in)\n",
    "                    self.signal_in = self.signal_forward[f'{key}']\n",
    "                elif 'Softmax' in key:\n",
    "                    \n",
    "                    self.signal_in = self.signal_in.T\n",
    "                    self.signal_in = self.signal_in - numpy.max(self.signal_in, axis=0)\n",
    "                    y = numpy.exp(self.signal_in) / numpy.sum(numpy.exp(self.signal_in), axis=0)\n",
    "                    self.signal_forward[f'{key}'] = y.T\n",
    "\n",
    "                    self.signal_in = self.signal_forward[f'{key}']\n",
    "\n",
    "        return self.signal_in\n",
    "\n",
    "    def calc_cross_entropy_error(self,prediction,label):\n",
    "        assert BATCH_SIZE == len(label)\n",
    "        loss  = 0\n",
    "        \n",
    "        for i in range(BATCH_SIZE):\n",
    "            loss += 1*numpy.log10(prediction[i][label[i]]+1e-6)\n",
    "\n",
    "        loss = -1*loss/BATCH_SIZE\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def back_propagation(self, prediction, label):\n",
    "\n",
    "        # LETS ASSUME THE ERROR IS CROSS_ENTROPY_ERROR\n",
    "        # AND THE LAST ACTIVATION IS ALWAYS SOFTMAX LAYER\n",
    "\n",
    "            # ONE-HOT\n",
    "        t = numpy.zeros_like(prediction)\n",
    "        for row in range(BATCH_SIZE):\n",
    "            t[row][label[row]] = 1\n",
    "\n",
    "        y = numpy.array(prediction - t)\n",
    "\n",
    "        # we will neglect the last soft-max layer by calling layers except the last layer\n",
    "        for layer_lv in range(len(self.model_layer.keys())-2,-1,-1):\n",
    "            current_layer = list(self.model_layer.keys())[layer_lv]\n",
    "\n",
    "            if 'input' in current_layer:\n",
    "                # INPUT IS NOT UPDATED BY GRADIENT\n",
    "                pass\n",
    "\n",
    "            elif 'dense' in current_layer:\n",
    "                self.signal_backward[current_layer] = [[],[]] # one for weight, the other for bias\n",
    "\n",
    "                upper_layer = list(self.model_layer.keys())[layer_lv-1]\n",
    "                \n",
    "                x = self.signal_forward[upper_layer]\n",
    "                \n",
    "                dy = numpy.matmul(numpy.transpose(x), y)\n",
    "\n",
    "                self.signal_backward[current_layer][0] = dy\n",
    "                self.signal_backward[current_layer][1] = numpy.sum(y,0)\n",
    "                self.signal_backward[current_layer][1] = numpy.expand_dims(self.signal_backward[current_layer][1], axis=0)\n",
    "\n",
    "                y  = numpy.matmul(y,numpy.transpose(self.model_layer[current_layer][0]))\n",
    "                \n",
    "            elif 'activation' in current_layer:\n",
    "                if 'Relu' in current_layer:\n",
    "                    self.signal_backward[current_layer] = []\n",
    "\n",
    "                    upper_layer = list(self.model_layer.keys())[layer_lv-1]\n",
    "\n",
    "                    x = self.signal_forward[upper_layer]\n",
    "                    _x = numpy.zeros_like(x)\n",
    "\n",
    "                    dy = y*numpy.not_equal(x,_x)\n",
    "                   \n",
    "                    self.signal_backward[current_layer] = dy\n",
    "\n",
    "                    y = dy\n",
    "\n",
    "    def update_gradient(self, lr=LEARNING_RATE):\n",
    "        \n",
    "        for key in self.model_layer.keys():\n",
    "            if 'dense' in key:\n",
    "                self.model_layer[key][0] = self.model_layer[key][0] - lr*self.signal_backward[key][0] # weight update\n",
    "                self.model_layer[key][1] = self.model_layer[key][1] - lr*self.signal_backward[key][1] # bias update\n",
    "            \n",
    "    def fit(self, train_x, train_y, test_x, test_y, batch_size, n_epoach):\n",
    "\n",
    "        # FLATTEN\n",
    "        train_x = train_x.reshape(len(train_x),-1)\n",
    "        test_x = test_x.reshape(len(test_x),-1)\n",
    "\n",
    "        iteration = len(train_x)//batch_size\n",
    "        test_iteration = len(test_x)//batch_size\n",
    "        \n",
    "        test_set_sample_index = numpy.random.randint(len(test_x))\n",
    "        \n",
    "        for nth_epoach in range(n_epoach):\n",
    "            loss_list = list()\n",
    "            test_acc = list()\n",
    "            print(f\"------------------------ epoch : {nth_epoach+1} --------------------------------\",end=\"\\n\")\n",
    "            for nth_iteration in range(iteration):\n",
    "                print(f\"iteration : {nth_iteration}/{iteration}\",end='\\r')\n",
    "                #normalization and batch split\n",
    "                train_x_sub = train_x[nth_iteration*batch_size:(nth_iteration+1)*batch_size]/255.0\n",
    "                train_y_sub = train_y[nth_iteration*batch_size:(nth_iteration+1)*batch_size]\n",
    "                loss_list.append([self.calc_cross_entropy_error(self.predict(train_x_sub), train_y_sub)])\n",
    "                self.back_propagation(self.predict(train_x_sub),train_y_sub)\n",
    "                self.update_gradient(lr=LEARNING_RATE*LEARNING_RATE_DECAY**nth_epoach)\n",
    "            \n",
    "            for nth_t_iteration in range(test_iteration):\n",
    "                test_x_sub = test_x[nth_t_iteration*batch_size:(nth_t_iteration+1)*batch_size]/255.0\n",
    "                test_y_sub = test_y[nth_t_iteration*batch_size:(nth_t_iteration+1)*batch_size]\n",
    "                test_acc.append([self.calc_acc(test_x_sub, test_y_sub)])\n",
    "                \n",
    "            print(f\"training_loss : {round(numpy.mean(loss_list),4)}\")\n",
    "            print(f\"validation_acc : {round(numpy.mean(test_acc),4)}\",end='\\n')\n",
    "\n",
    "    def calc_acc(self,valid_x, valid_y):\n",
    "        y= self.predict(valid_x)\n",
    "        \n",
    "        prediction = numpy.argmax(y,1)\n",
    "        mask = numpy.ones_like(prediction)\n",
    "        truth = numpy.equal(prediction,valid_y)\n",
    "        \n",
    "        acc = numpy.sum((mask*truth),0)/BATCH_SIZE\n",
    "        # print(\"y : \", y)\n",
    "        # print(\"prediction : \", prediction)\n",
    "        # print(\"answer : \", valid_y)\n",
    "        # print(\"check: \", mask*truth)\n",
    "        # assert False\n",
    "        return acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Build:</b> The depth and the size of dense layers and relu layers can be modified. Just make sure Input_layer comes first, and the ONLY softmax layer comes last</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequentaial_model(\n",
    "    Input_layer(x_train),\n",
    "    Dense_layer(128),\n",
    "    Activation_layer('Relu'),\n",
    "    Dense_layer(10),\n",
    "    Activation_layer('Softmax')\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> There is no appropriate weight initialization, batch normalization nor gradient optimization in this model. Try different variables and layer combinations to necessity of these important features that are mentioned.(ex: large learning rate, small node size)</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ epoch : 1 --------------------------------\n",
      "training_loss : 3.9805\n",
      "validation_acc : 0.1032\n",
      "------------------------ epoch : 2 --------------------------------\n",
      "training_loss : 2.3923\n",
      "validation_acc : 0.4111\n",
      "------------------------ epoch : 3 --------------------------------\n",
      "training_loss : 1.5182\n",
      "validation_acc : 0.6773\n",
      "------------------------ epoch : 4 --------------------------------\n",
      "training_loss : 1.8048\n",
      "validation_acc : 0.6461\n",
      "------------------------ epoch : 5 --------------------------------\n",
      "training_loss : 1.476\n",
      "validation_acc : 0.7744\n",
      "------------------------ epoch : 6 --------------------------------\n",
      "training_loss : 1.4574\n",
      "validation_acc : 0.7778\n",
      "------------------------ epoch : 7 --------------------------------\n",
      "training_loss : 1.4142\n",
      "validation_acc : 0.7778\n",
      "------------------------ epoch : 8 --------------------------------\n",
      "training_loss : 1.4062\n",
      "validation_acc : 0.7911\n",
      "------------------------ epoch : 9 --------------------------------\n",
      "training_loss : 1.4241\n",
      "validation_acc : 0.7783\n",
      "------------------------ epoch : 10 --------------------------------\n",
      "training_loss : 1.4212\n",
      "validation_acc : 0.7849\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, x_test,y_test, batch_size=BATCH_SIZE, n_epoach=NUMBER_OF_EPOACHES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep_Scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a4b549291e0e86b191a7dcb7a5876a76793a5689e1e99b0448ba3502e0564c05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
